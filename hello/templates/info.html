{% extends "base.html" %}
{% load staticfiles %}

{% block content %}

<script type="text/javascript">
    var graphOptions = {
        series: {
            lines: {
                show: true
            },
            points: {
                show: false
            }
        },
        grid: {
            hoverable: false
        },
        xaxis: {
            axisLabel: 'Predicted Probability',
            min: 0,
            max: 100
        },
    }
    
    var diagonalLine = [], horizontalLine = [];
    
    for (var i = 0; i < 100; i ++){
        diagonalLine.push([i,i]);
        horizontalLine.push([i,0]);
    }

    $(document).ready(function () {
        $(function() {
            graphOptions['yaxis'] = {'axisLabel': 'Observed Probability'};
            $.plot("#pollData",
                [ 
                    {'data':{{ chartsData.0 }}, 'label':' 538 Poll Only', 'color':'blue'},
                    {'data':{{ chartsData.2 }}, 'label':' 538 Poll Plus', 'color':'green'},
                    {'data':{{ chartsData.4 }}, 'label':' PredictWise', 'color':'red'},
                    {'data': diagonalLine, 'label':' Theoretical'}
                ],
                graphOptions
            );
            
            graphOptions['yaxis'] = {'axisLabel': 'Observed Probability Deviations'};
            $.plot("#pollDiffData",
                [
                    {'data':{{ chartsData.1 }}, 'label':' 538 Poll Only', 'color':'blue'},
                    {'data':{{ chartsData.3 }}, 'label':' 538 Poll Plus', 'color':'green'},
                    {'data':{{ chartsData.5 }}, 'label':' PredictWise', 'color':'red'},
                    {'data': horizontalLine, 'label':' Theoretical'}
                ],
                graphOptions
            );
        });
    });
</script>

<div class="jumbotron text-center">
  <div class="container">
    <h1>538 vs Prediction Markets</h1>
    <h2>How accurate is 538?</h2>
  </div>
</div>
<div class="container infoPage">
  <div class="row">
  
    <div class="col-md-8 col-centered">

        <article>
            <p><b><a href="http://projects.fivethirtyeight.com/election-2016/primary-forecast/" target="_blank">FiveThirtyEight (538)</a> is well-known for its accurate predictions on political elections. But exactly how accurate is it? To find out, we answer these 2 questions:</b></p>
            
            <h2>1. Which 538 model is more accurate? Poll-only or poll-plus?</h2>
            
            <p>538 uses two models to give predictions: a “poll-only” model and a “poll-plus” model which accounts for factors in addition to polls such as demographics and historical trends.</p>
            
            <p><i>We found that poll-plus appears to be better so far.</i></p>
            
            <h2>2. How does 538 compare to prediction markets?</h2>
            
            <p>In addition to 538, people often look to prediction markets for political forecasts as well. While 538's 50/50 perfect score during the 2012 presidential election is impressive, he could have gotten 49/50 if he simply bet according to prediction markets (InTrade was the most popular site back then). The only toss-up was Florida.</p>
            
            <p><i>We found that it appears 538 fared no better than the prediction markets.</i></p>

        </article>
            
        <article>
        
            <h1>Analytic Metrics</small></h1>
            
            <p>To answer the above questions, we performed the following tests.<b>The prediction markets are consolidated by <a href="http://predictwise.com/faq/" target="_blank">PredictWise (PW)</a>.</b></p>
            
            <h3>Metric A.</h3>
            <p>Whenever the predictor, be it 538 or PW, says some event will occur with probability P, what is the actual observed probability P'? If we plot P' against P, we expect a linear relationship P'=P for a perfect predictor given sufficient amount of data. We can tell how good the predictor is by observing how close it is to this line.</p>
            
            <p>Results of Metric A are shown below.</p>
            
            <h3>Metric B.</h3>
            <p>Another more concrete way of evaluating 538's accuracy is to use the 538 values as a reference to bet in the prediction markets. At any given time when 538's predicted value is above PW's value, we will <a href="https://betting.betfair.com/what-is-lay-betting.html" target="_blank"><b>back</b></a> the contract. Otherwise, we will <a href="https://betting.betfair.com/what-is-lay-betting.html" target="_blank"><b>lay</b></a> the contract.</p>
            
            <p><a href="/">Our front page show the results of Metric B.</a> The money spent and gained shown below each chart summarizes your total win/loss if you had bet this way. We ignore any transaction costs or bid/ask spread.</p>
        
        </article>
        
        <article>
            <h1>Data for Running the Metrics</h1>
            
            <p>We took our data from the 2016 primaries for both parties. PW has almost all the data sets, missing only a few, whereas 538 only gave predictions for about 60% of them due to lack of polling data, Of course, Metric B can only be applied to the states where both the 538 and PW data are available.</p> 
            
            <p>Note that currently all our analysis are based on very limited data. Our results may change with more data in the future.</p>

        </article>
        
        <article id="metricA">
        
            <h1>Metric A Results. Comparing accuracy of predictions</h1>
            
            <p>Running through the data, we produced the following charts, which tells us <b>PW seems to fit closest to the theoretical line, followed by 538 poll-plus, and 538 poll-only comes last.</b></p>
            
            <div id="pollData" class="infoGraphs"></div>
            
            <p>This first graphs P' (the observed probability) vs P(the predicted probability). The theoretical result is a straight line from (0,0) to (100%, 100%). As seen, the 538 poll-only seems to fluctuate the most wildly from the theoretical line, followed by 538 poll-plus, then PW. However, it has to be noted that PW enjoys the benefit of having more data sets. If we restrict PW to those states that 538 have a prediction on (this graph is not show in the figure), then the results of PW-restricted are similar to that of 538 poll-plus.</p>
            
            <div class="expandsection">
                <p class="hoverover">More details (hover to read)</p>
                
                <div class="moreinfo">
                
                    <p>Note: we have chosen to weight each data set differently. If we treat each data point equally regardless of which set they come from, certain state results dominate since they have a lot more points than others. For 538 it ranges wildly from only a few dozen data points to more than 700. On the other hand, if we weight each dataset equally, regardless of how many data points are in it, we also over-represent certain predictions. To resolve this, we chose to use log(length) as the weight.</p>
                    
                </div>
            </div>    

            <div id="pollDiffData" class="infoGraphs"></div>
            
            <p>The second graph shows basically the same thing except as a deviation from the theoretical value.</p>
            
            <p>If we compute the mean and standard deviation of the fluctuations, we find:</p>
            
            <p>538 poll-only:  mean: 2.94/ standard deviation: 16.88</p>
            
            <p>538 poll-plus: mean: 1.48/ standard deviation: 12.95</p>
            
            <p>PW: mean: 0.85/ standard deviation: 9.33</p>
            
            <p>PW-restricted: mean: 1.04/ standard deviation 12.21</p>
        
        </article>
        
        <article id="metricBpartI">
            <h1>Metric B Results Part I. If I bet on a party's primary for a particular state according to 538. Would I win or lose?</h1>
            
            <p><b><a href="/">The results are shown on the home page.</a></b></p>
            
            <p>We use the 538 value as a guide to decide whether we <a href="https://betting.betfair.com/what-is-lay-betting.html" target="_blank">back or lay a contract</a> on PW.</p>
            
            <p>As an example: Suppose Clinton-Win on PW is currently at 70%, but on 538 it is 82%. In this case, if we believe 538 to be more accurate than PW, we would consider Clinton-Win to be under-priced. Therefore we choose to back (buy) this contract at the PW market value, which is $70. If she ultimately wins, we get $100, if she loses, we get $0.</p>
            
            <p>On the flip side, if 538 reports Clinton-Win at anything below 70%, then we lay Clinton on PW. The lay contract will cost us $30. If Clinton wins, we get $0, if she loses, we get $100.</p>

            <p>We have ignored all transaction costs and bid/ask spread in this calculation.</p>
                  
        </article>
        
        <article id="metricBpartII">
        
            <h1>Metric B Results Part II. If I bet on all the possible state primaries according to 538. Would I win or lose overall?</h1>
            
            <p>We have summed up the gain/loss from every state where it is applicable, again weighted by log(length) of the number of times where 538 and PW report values at the same time. We get that:</p>
            
            <p>If betting according to 538 poll-only: For every 305.3 dollar spent, you gain 348.1 back. <b>For a net profit of 42.8.</b></p>
            
            <p>If betting according to 538 poll-plus: For every 343.9 dollar spent, you gain 327.9 back. <b>For a net loss of 16.</b></p>
            
            <p>It has to be noted that our Metric B (the primaries) doesn't necessarily imply 538 poll-only is more accurate than 538 poll-plus, or even more accurate than PW. Our metric is a bit subtle</p>

            <p>If 538 is truly accurate, and PW is not, then betting on PW with guidance from 538 will result in gains on average. However, if PW is truly accurate, then no matter how good/bad 538 is, you will gain exactly 0 profit on average. The reasoning being that if PW is already priced accurately, then there's no way for you to beat the “already true” odds.</p>
            
            <p>It is also possible for PW to be “more accurate” than 538 according to the first metric, while still allowing 538 to make money off of PW on average.</p>
            
            <p>As a contrived example, suppose both 538 and PW are betting on the outcome of a fair coin. In this case, the true odds is always 50%. Imagine if PW always predicts 49%, while 538 predicts 60%. Even though PW is more accurate by the first metric, 538 will end up making money because it signals us to buy the contract at 49%, resulting in gains on average.</p>
            
            <p>Nevertheless, all of our conclusions should be taken with a grain of salt because we still don't have enough data. We are hoping to continue update the results of the analysis after the general election is over, where we expect the total number of data points for 538 to more than double.</p>
            
        </article>
        
        <article>
            
        <article>
        
            <div class="expandsection">
                <p class="hoverover">First writeup (hover to read)</p>
                
                <div class="moreinfo">
        
<pre>    
Source of data:


TL;DR: We use the primary results for both parties. Naively there are 100 data sets available (50 states and two parties each). In reality, 2 states (CO and ND) held no GOP primary. In addition, WY_R, VT_R are missing from PW, but it does include the data for the Republican Puerto Rico primary. For a total of 97. 538 on the other hand are missing more data, they only gave predictions for 59 states.


################

We will use the data from the 2016 presidential primaries for both parties. The prediction market data is taken from predictwise, which is in turn derived from betfair, a website where people can actually bet money against each other on the outcome of each primary. There are 99 such data available. Two states did not have a GOP primary, and we have one additional result from Puerto Rico. On the 538 side, there are 61 results available. The are fewer here because 538 did not choose to give a prediction on states with insufficient polling data.

538 is currently making predictions for every state on the outcome of the general election. Therefore, there will be another surge of data after 11/8. On the prediction market side, there are similar data for each state on predictit.

We are not only looking at the end result of the prediction, we will take into account the entire history of each predicted probability of winning.

=========================================
Data and metric


TL;DR:

1. If a predictor is truly accurate, the property we want to test is that: among all instances when it says something will happen with probability P, the actual probability of the events actually happening should be close to P. Note that this includes all instances in the past when the predictor made the prediction at P, so a single chart could in principle provide many instances at P for testing.

Given an event, and a chart of predicted values over time that this event will occur. We will represent the predicted data on a line from 0% to 100%. If the event actually occurred, we mark white dots at all values that the predictor made a prediction. If the event didn't actually occur, we color the dots black instead. Over many events and many dots, we should see a trend where close to the 0% side of the line, there should be more black dots, and there should be more white dots closer to the 100% end of the line.

We can plot the percentage of white dots in a small segment of the line. As we move the segment towards the right, we expect to see the percentage of white dots increase. For an ideal predictor over many events, we expect a linear increase of the white dot percentage from 0% to 100% as we move across. We plot 

2.We can perform another test as follows:

We will pretend to buy/sell contracts on the predictions market based on the 538 prediction. If the 538 prediction is lower, we buy, otherwise we sell. The expected result depends on the assumptions as follows:

If 538 is truly accurate, and PW is not: then on average we would expect a positive return.

If PW is truly accurate: then no matter what 538 says, we expect to neither gain or lose. In fact we can buy/sell based on random coin tosses and still expect to make exact 0 profit, since the PW values are already truly accurate, there's no room to make positive expected returns.

#################################

In order to decide which predictor is more accurate, we need a metric. Let's describe it here.

All the history of the winning probability counts – For each state, both 538 and predictwise gives a prediction periodically. Usually one per day for 538, and multiple per day for predictwise, as the market price for each prediction changes from moment to moment. We will use all of them.

Expected behavior for an accurate predictor. If at any given moment the predictor says something will happen with 60% probability, then, if the predictor is accurate, among all instances of such a predictor giving a 60% prediction, the event will actually happen with close to 60% probability. This leads us to the following idea. Go through all the data and find all instances of the predictor giving a probability P of an event happening. Then calculate the actual result P'. An accurate predictor will have P' very close to P after sufficient amount of data.

We can imagine we have a line segment marked from 0% to 100%. We first go through our first data set containing a history of predictions of an event. WOLOG let's say in the end it turns out this event actually happened. Then, every time this predictor gives a prediction of P%, we mark the spot on the line segment at P%. We will mark this point white if the event actually happened, and black if it didn't turn out to happen. We then do the same for all the data sets.

After many many data sets, if the predictor is any good, then we expect the left end of the line segment (the part with low probability predictions) to be mostly black, while the right end is mostly white. There will also be a gradual transition from black to white as one moves from left to right.

If we take a small slice of the line segment at, say, 60%, we will find a number of white points and black points. If the predictor is accurate, we expect to see roughly 60% white dots and 40% black dots. We can plot the deviation from the expected value across the line segment at any slice.

Therefore, a good predictor should have deviation values that are close to 0, and no systematic trends.

====================
Subtleties

In reality, simply weighing each point the same is not a good idea because some data sets for a particular state are a few orders of magnitude larger than others. If we choose a uniform weighing, then a few states' results will end up dominating the distribution of white/black points on the line segment. Therefore we need to choose a weighing scheme.

Simply making each state results have the same combined total weight is also not good because some states have less than 10 data points, while others have several hundred. We compromise by choosing the use the log of the number of points as the total weight of each state. This choice is completely arbitrary.

Another subtlety is the time data given by 538. Whilte PW has timestamped their data up to the minute, 538 does so only up to each day. If we naively convert to epoch time, it would make it appear that 538 made the prediction at 0:00am each day, which will make it look more advanced.

================
Result

We were able to perform the analysis for 538poll, 538plus, and PW. Also, since PW results have more data sets than 538 (99 vs 61), we also choose to do the analysis of PW on the restricted subset of states that 538 chose to use. We will call this PW_restricted.

We can clearly see that PW seems to have the smallest deviations from the expected value. While PW_restricted and 538plus are very similar. 538Poll is the worst.

We looked at small neighborhoods around every 2% interval for a total of 50 intervals. We then calculate the mean and standard deviation of the difference to the expected result in these intervals. We get that:

538poll: 2.94/16.88
538plus: 1.48/12.95
PW: 0.85/9.33
PW_restricted 1.04/12.21

However, it has to be mentioned that the amount of data we have is still not conclusive enough to provide a definitive answer to which predictor is the most accurate. So far PW is leading, but it could be simply due to having more data sets to work with. It would be interesting to see the results after the general election.

==================
Simulations:

To get a better feel of what a “truly accurate” predictor (from now referred to as the oracle) behaves like. We can make predictions on the following game.

We toss the coin 201 times, and after each toss, we give a prediction on the probability of heads coming up more than tails. Notice that this probability is exactly calculable. So our predictor is exactly accurate. Given sufficient number of trials, the predicted values at any slice will approach the true value.

Indeed we see this behavior after a large number of trials. Additionally, for small number of trials (60-100), we can see how likely it is to have deviations as large as the ones given by 538 or predictwise. It does seem like the deviations from both 538/predictwise are a bit too large. This is reasonable because many of the states are not contested. We would simply have a line staying very close to 100% or 0%. Either way, it won't contribute to any data points in the middle. Thus, the true number of “useful” data sets is much smaller than 65/99.

We can also use the simulated results to get a better idea on how the standard deviation is supposed to change as we do more trials, so we can assess whether the improved results on PW compared to 538 is solely due to having access to more data.
</pre>
                </div>
            </div>
                    
        
        </article>
        
    </div>
  </div>
</div>  

<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-84582463-1', 'auto');
  ga('send', 'pageview');

</script>
{% endblock %}
